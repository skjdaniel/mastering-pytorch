{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune AlexNet to classify bees and ants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune AlexNet on the [Hymenoptera dataset](https://www.kaggle.com/datasets/ajayrana/hymenoptera-data) from Kaggle.\n",
    "\n",
    "The training set has 124 images of ants and 121 images of bees. One image in the training set labelled as an image of an ant just says \"No image found\". I removed this image from the training set, so we're left with 123 images of ants. \n",
    "\n",
    "The validation set has 70 images of ants and 83 of bees.\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Hymenoptera), the order Hymenoptera includes insects besides ants and bees (such as wasps). The Kaggle dataset only includes images of ants and bees though.\n",
    "\n",
    "The Mastering Pytorch text references the same Kaggle site as the source of the data, but says that there are 240 training images and 150 validation images, equally split between the two classes. I don't know why there's a difference. \n",
    "\n",
    "I freeze all of the parameters of the pretrained model except for the last two linear layers of the classifier. \n",
    "\n",
    "- Imports.\n",
    "- Calculate the mean and std for data normalization.\n",
    "- Datasets and dataloaders. The datasets are labeled train and val, but the val set is really a test set.\n",
    "- Download pretrained AlexNet model. Modify the last layer so that it's suitable for 2 (rather than 10) classes. Freeze all parameters except for those of the last two linear layers. This results in around ~16.8 million trainable parameters.\n",
    "- Define a function to train the model.\n",
    "- Define the device (cpu), optimizer, and loss function.\n",
    "- Briefly train the model.\n",
    "\n",
    "There's no hyperparameter tuning here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, reduce\n",
    "\n",
    "path = 'data/hymenoptera_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean and standard deviation for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untransformed_train_dataset = datasets.ImageFolder(os.path.join(path, 'train'),\n",
    "                                  transform=transforms.ToTensor())\n",
    "len(untransformed_train_dataset)                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5172, 0.4753, 0.3484]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_sizes = torch.empty(len(untransformed_train_dataset), dtype=torch.int32)\n",
    "image_sums = torch.empty((len(untransformed_train_dataset), 3), dtype=torch.float32)\n",
    "\n",
    "for idx, (image, _) in enumerate(untransformed_train_dataset):\n",
    "    image = rearrange(image, 'c h w -> c (h w)')\n",
    "    image_sum = reduce(image, 'c x -> c', 'sum')\n",
    "    image_sizes[idx] = image.shape[-1]\n",
    "    image_sums[idx] = image_sum\n",
    "\n",
    "total_sum = reduce(image_sums, 'n c -> c', 'sum')\n",
    "total_size = reduce(image_sizes, 'n -> ()', 'sum')\n",
    "means = total_sum / rearrange(total_size, '() -> () 1') \n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2776, 0.2575, 0.2865]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_vars = torch.empty((len(untransformed_train_dataset), 3), dtype=torch.float32)\n",
    "for idx, (image, _) in enumerate(untransformed_train_dataset):\n",
    "    image = rearrange(image, 'c h w -> c (h w)')\n",
    "    image -= rearrange(means, '1 c -> c 1')\n",
    "    image *= image\n",
    "    image_sum = reduce(image, 'c x -> c', 'sum')\n",
    "    image_vars[idx] = image_sum\n",
    "\n",
    "total_var = reduce(image_vars, 'n c -> c', 'sum')\n",
    "std_sqs = total_var / rearrange(total_size, '() -> () 1')  \n",
    "stds = torch.sqrt(std_sqs)\n",
    "stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5172, 0.4753, 0.3484]), tensor([0.2776, 0.2575, 0.2865]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = rearrange(means, '1 c -> c')\n",
    "stds = rearrange(stds, '1 c -> c')\n",
    "means, stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text uses `[0.490, 0.449, 0.411]` and `[0.490, 0.449, 0.411]` for the channel means and standard deviations respectively. I don't know where these numbers come from. But it seems we're using different datasets, so it makes sense to find different means and standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants', 'bees']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transformers = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)\n",
    "    ])\n",
    "}\n",
    "image_datasets = {phase: datasets.ImageFolder(os.path.join(path, phase), data_transformers[phase]) for phase in ['train', 'val']}\n",
    "\n",
    "data_loaders = {phase: DataLoader(image_datasets[phase], batch_size=8,\n",
    "    shuffle=True) for phase in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {phase: len(image_datasets[phase]) for phase in ['train', 'val']}\n",
    "\n",
    "classes = image_datasets['train'].classes\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying pretrained model; freezing most parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (7): ReLU(inplace=True)\n",
      "  (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (9): ReLU(inplace=True)\n",
      "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "avgpool AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "classifier Sequential(\n",
      "  (0): Dropout(p=0.5, inplace=False)\n",
      "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Dropout(p=0.5, inplace=False)\n",
      "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "alexnet_to_finetune = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "\n",
    "for name, module in alexnet_to_finetune.named_children():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 16,789,506 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "alexnet_to_finetune.classifier[6] = nn.Linear(4096, 2)\n",
    "\n",
    "for param in alexnet_to_finetune.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in alexnet_to_finetune.classifier.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for classifier_layer in [4, 6]:\n",
    "    for param in alexnet_to_finetune.classifier[classifier_layer].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(alexnet_to_finetune):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fine-tune pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetine_model(model, device, dataloader, optimizer, \n",
    "                    loss_function, epochs=10):\n",
    "    start_time = time.time()\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        print('-'*10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.\n",
    "            running_corrects = 0\n",
    "\n",
    "            for images, labels in dataloader[phase]:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    prediction_probs = model(images)\n",
    "                    loss = loss_function(prediction_probs, labels)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                predictions = prediction_probs.argmax(dim=1)\n",
    "                batch_correct = predictions.eq(labels).sum().item()\n",
    "                running_corrects += batch_correct\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase] \n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}') \n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(f'Training complete in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define device, optimizer, and loss function, and set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2521869c3b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.SGD(alexnet_to_finetune.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "train Loss: 0.0623 Acc: 0.7787\n",
      "val Loss: 0.0403 Acc: 0.8693\n",
      "Epoch 2/10\n",
      "----------\n",
      "train Loss: 0.0427 Acc: 0.8566\n",
      "val Loss: 0.0322 Acc: 0.9085\n",
      "Epoch 3/10\n",
      "----------\n",
      "train Loss: 0.0336 Acc: 0.9098\n",
      "val Loss: 0.0490 Acc: 0.8889\n",
      "Epoch 4/10\n",
      "----------\n",
      "train Loss: 0.0350 Acc: 0.8934\n",
      "val Loss: 0.0275 Acc: 0.9216\n",
      "Epoch 5/10\n",
      "----------\n",
      "train Loss: 0.0257 Acc: 0.9139\n",
      "val Loss: 0.0280 Acc: 0.9150\n",
      "Epoch 6/10\n",
      "----------\n",
      "train Loss: 0.0291 Acc: 0.8975\n",
      "val Loss: 0.0292 Acc: 0.8954\n",
      "Epoch 7/10\n",
      "----------\n",
      "train Loss: 0.0277 Acc: 0.9139\n",
      "val Loss: 0.0286 Acc: 0.9150\n",
      "Epoch 8/10\n",
      "----------\n",
      "train Loss: 0.0242 Acc: 0.9426\n",
      "val Loss: 0.0269 Acc: 0.9085\n",
      "Epoch 9/10\n",
      "----------\n",
      "train Loss: 0.0233 Acc: 0.9344\n",
      "val Loss: 0.0272 Acc: 0.9150\n",
      "Epoch 10/10\n",
      "----------\n",
      "train Loss: 0.0184 Acc: 0.9631\n",
      "val Loss: 0.0265 Acc: 0.9150\n",
      "Training complete in 1m 38s\n"
     ]
    }
   ],
   "source": [
    "finetine_model(model=alexnet_to_finetune, device=device, \n",
    "        dataloader=data_loaders, optimizer=optimizer,\n",
    "        loss_function=loss_function, epochs=10) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastering_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
