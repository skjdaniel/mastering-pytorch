{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune AlexNet to classify bees and ants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I fine-tune AlexNet on the ![Hymenoptera dataset](https://www.kaggle.com/datasets/ajayrana/hymenoptera-data) from Kaggle.\n",
    "\n",
    "The training set has 124 images of ants and 121 images of bees. One image in the training set labelled as an image of an ant just says \"No image found\". I removed this image from the training set, so we're left with 123 images of ants. \n",
    "\n",
    "The validation set has 70 images of ants and 83 of bees.\n",
    "\n",
    "According to ![Wikipedia](https://en.wikipedia.org/wiki/Hymenoptera), the order Hymenoptera includes insects besides ants and bees (such as wasps). The Kaggle dataset only includes images of ants and bees though.\n",
    "\n",
    "The Mastering Pytorch text references the same Kaggle site as the source of the data, but says that there are 240 training images and 150 validation images, equally split between the two classes. To be honest I don't know why there seems to be a difference. \n",
    "\n",
    "I freeze all of the parameters of the pretrained model except for the last two linear layers of the classifier. Just because I'm a bit lazy and this doesn't seem too interesting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, reduce\n",
    "\n",
    "path = '../data/hymenoptera_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean and standard deviation for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untransformed_train_dataset = datasets.ImageFolder(os.path.join(path, 'train'),\n",
    "                                  transform=transforms.ToTensor())\n",
    "len(untransformed_train_dataset)                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5172, 0.4753, 0.3484]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_sizes = torch.empty(len(untransformed_train_dataset), dtype=torch.int32)\n",
    "image_sums = torch.empty((len(untransformed_train_dataset), 3), dtype=torch.float32)\n",
    "\n",
    "for idx, (image, _) in enumerate(untransformed_train_dataset):\n",
    "    image = rearrange(image, 'c h w -> c (h w)')\n",
    "    image_sum = reduce(image, 'c x -> c', 'sum')\n",
    "    image_sizes[idx] = image.shape[-1]\n",
    "    image_sums[idx] = image_sum\n",
    "\n",
    "total_sum = reduce(image_sums, 'n c -> c', 'sum')\n",
    "total_size = reduce(image_sizes, 'n -> ()', 'sum')\n",
    "means = total_sum / rearrange(total_size, '() -> () 1') \n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2776, 0.2575, 0.2865]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_vars = torch.empty((len(untransformed_train_dataset), 3), dtype=torch.float32)\n",
    "for idx, (image, _) in enumerate(untransformed_train_dataset):\n",
    "    image = rearrange(image, 'c h w -> c (h w)')\n",
    "    image -= rearrange(means, '1 c -> c 1')\n",
    "    image *= image\n",
    "    image_sum = reduce(image, 'c x -> c', 'sum')\n",
    "    image_vars[idx] = image_sum\n",
    "\n",
    "total_var = reduce(image_vars, 'n c -> c', 'sum')\n",
    "std_sqs = total_var / rearrange(total_size, '() -> () 1')  \n",
    "stds = torch.sqrt(std_sqs)\n",
    "stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5172, 0.4753, 0.3484]), tensor([0.2776, 0.2575, 0.2865]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = rearrange(means, '1 c -> c')\n",
    "stds = rearrange(stds, '1 c -> c')\n",
    "means, stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text uses `[0.490, 0.449, 0.411]` and `[0.490, 0.449, 0.411]` for the channel means and standard deviations respectively. I don't know where these numbers come from. But it seems we're using different datasets, so it makes sense to find different means and standard deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants', 'bees']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transformers = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)\n",
    "    ])\n",
    "}\n",
    "image_datasets = {phase: datasets.ImageFolder(os.path.join(path, phase), data_transformers[phase]) for phase in ['train', 'val']}\n",
    "\n",
    "data_loaders = {phase: DataLoader(image_datasets[phase], batch_size=8,\n",
    "    shuffle=True) for phase in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {phase: len(image_datasets[phase]) for phase in ['train', 'val']}\n",
    "\n",
    "classes = image_datasets['train'].classes\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying pretrained model; freezing most parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (7): ReLU(inplace=True)\n",
      "  (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (9): ReLU(inplace=True)\n",
      "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "avgpool AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "classifier Sequential(\n",
      "  (0): Dropout(p=0.5, inplace=False)\n",
      "  (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "  (2): ReLU(inplace=True)\n",
      "  (3): Dropout(p=0.5, inplace=False)\n",
      "  (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (5): ReLU(inplace=True)\n",
      "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "alexnet_to_finetune = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "\n",
    "for name, module in alexnet_to_finetune.named_children():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 16,789,506 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "alexnet_to_finetune.classifier[6] = nn.Linear(4096, 2)\n",
    "\n",
    "for param in alexnet_to_finetune.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in alexnet_to_finetune.classifier.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for classifier_layer in [4, 6]:\n",
    "    for param in alexnet_to_finetune.classifier[classifier_layer].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(alexnet_to_finetune):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fine-tune pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetine_model(model, device, dataloader, optimizer, \n",
    "                    loss_function, epochs=10):\n",
    "    start_time = time.time()\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        print('-'*10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.\n",
    "            running_corrects = 0\n",
    "\n",
    "            for images, labels in dataloader[phase]:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    prediction_probs = model(images)\n",
    "                    loss = loss_function(prediction_probs, labels)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                predictions = prediction_probs.argmax(dim=1)\n",
    "                batch_correct = predictions.eq(labels).sum().item()\n",
    "                running_corrects += batch_correct\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase] \n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}') \n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(f'Training complete in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define device, optimizer, and loss function, and set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28110fdc3b0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.SGD(alexnet_to_finetune.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model (10 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "train Loss: 0.0620 Acc: 0.7705\n",
      "val Loss: 0.0453 Acc: 0.8627\n",
      "Epoch 2/10\n",
      "----------\n",
      "train Loss: 0.0443 Acc: 0.8648\n",
      "val Loss: 0.0362 Acc: 0.8954\n",
      "Epoch 3/10\n",
      "----------\n",
      "train Loss: 0.0313 Acc: 0.9016\n",
      "val Loss: 0.0506 Acc: 0.8693\n",
      "Epoch 4/10\n",
      "----------\n",
      "train Loss: 0.0351 Acc: 0.8893\n",
      "val Loss: 0.0326 Acc: 0.9020\n",
      "Epoch 5/10\n",
      "----------\n",
      "train Loss: 0.0269 Acc: 0.9139\n",
      "val Loss: 0.0323 Acc: 0.9216\n",
      "Epoch 6/10\n",
      "----------\n",
      "train Loss: 0.0304 Acc: 0.9016\n",
      "val Loss: 0.0328 Acc: 0.9281\n",
      "Epoch 7/10\n",
      "----------\n",
      "train Loss: 0.0274 Acc: 0.9262\n",
      "val Loss: 0.0324 Acc: 0.9150\n",
      "Epoch 8/10\n",
      "----------\n",
      "train Loss: 0.0253 Acc: 0.9344\n",
      "val Loss: 0.0316 Acc: 0.9216\n",
      "Epoch 9/10\n",
      "----------\n",
      "train Loss: 0.0237 Acc: 0.9262\n",
      "val Loss: 0.0320 Acc: 0.9020\n",
      "Epoch 10/10\n",
      "----------\n",
      "train Loss: 0.0198 Acc: 0.9344\n",
      "val Loss: 0.0309 Acc: 0.9150\n",
      "Training complete in 1m 58s\n"
     ]
    }
   ],
   "source": [
    "finetine_model(model=alexnet_to_finetune, device=device, \n",
    "        dataloader=data_loaders, optimizer=optimizer,\n",
    "        loss_function=loss_function, epochs=10) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastering_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
